{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "file_path = \"../Json_File/Refined_file/Updated_Filtered_Quarterly_data.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies Clustered by Net Profit Digits:\n",
      "        Cluster                                Company\n",
      "2-Digit Cluster                          Birlasoft Ltd\n",
      "2-Digit Cluster                             Cyient Ltd\n",
      "2-Digit Cluster                Sasken Technologies Ltd\n",
      "2-Digit Cluster                    Sonata Software Ltd\n",
      "2-Digit Cluster                Zensar Technologies Ltd\n",
      "3-Digit Cluster                            MphasiS Ltd\n",
      "3-Digit Cluster Oracle Financial Services Software Ltd\n",
      "3-Digit Cluster                      Tech Mahindra Ltd\n",
      "4-Digit Cluster                              Wipro Ltd\n",
      "4-Digit Cluster          Tata Consultancy Services Ltd\n",
      "4-Digit Cluster                   HCL Technologies Ltd\n",
      "4-Digit Cluster                            Infosys Ltd\n",
      "  Other Cluster            R Systems International Ltd\n",
      "  Other Cluster             Kellton Tech Solutions Ltd\n",
      "  Other Cluster        Kernex Microsystems (India) Ltd\n",
      "  Other Cluster                             Mastek Ltd\n",
      "  Other Cluster                           Megasoft Ltd\n",
      "  Other Cluster                           Nettlinx Ltd\n",
      "  Other Cluster                Onward Technologies Ltd\n",
      "  Other Cluster                Palred Technologies Ltd\n",
      "  Other Cluster                 Persistent Systems Ltd\n",
      "  Other Cluster                 Saven Technologies Ltd\n",
      "  Other Cluster                      Tera Software Ltd\n",
      "  Other Cluster           Securekloud Technologies Ltd\n",
      "  Other Cluster                     Sofcom Systems Ltd\n",
      "  Other Cluster                      Softsol India Ltd\n",
      "  Other Cluster     Starcom Information Technology Ltd\n",
      "  Other Cluster              Kati Patang Lifestyle Ltd\n",
      "  Other Cluster                       Titan Intech Ltd\n",
      "  Other Cluster                Trigyn Technologies Ltd\n",
      "  Other Cluster                 USG Tech Solutions Ltd\n",
      "  Other Cluster                   VEDAVAAG Systems Ltd\n",
      "  Other Cluster                      WEP Solutions Ltd\n",
      "  Other Cluster                          SBSJHVHSV Ltd\n",
      "  Other Cluster       Jeevan Scientific Technology Ltd\n",
      "  Other Cluster                   Expleo Solutions Ltd\n",
      "  Other Cluster               Inspirisys Solutions Ltd\n",
      "  Other Cluster                        3i Infotech Ltd\n",
      "  Other Cluster                    Adroit Infotech Ltd\n",
      "  Other Cluster                AION-TECH SOLUTIONS Ltd\n",
      "  Other Cluster            Allied Digital Services Ltd\n",
      "  Other Cluster                   ASM Technologies Ltd\n",
      "  Other Cluster                Avance Technologies Ltd\n",
      "  Other Cluster                   Bartronics India Ltd\n",
      "  Other Cluster   Cambridge Technology Enterprises Ltd\n",
      "  Other Cluster        CG-Vak Software and Exports Ltd\n",
      "  Other Cluster                    COFORGE LIMITED Ltd\n",
      "  Other Cluster                               IZMO Ltd\n",
      "  Other Cluster      Cranes Software International Ltd\n",
      "  Other Cluster     Cybertech Systems and Software Ltd\n",
      "  Other Cluster          Danlaw Technologies India Ltd\n",
      "  Other Cluster         Datamatics Global Services Ltd\n",
      "  Other Cluster     Dynacons Systems and Solutions Ltd\n",
      "  Other Cluster                 Elnet Technologies Ltd\n",
      "  Other Cluster Equippp Social Impact Technologies Ltd\n",
      "  Other Cluster                Xchanging Solutions Ltd\n",
      "  Other Cluster             FCS Software Solutions Ltd\n",
      "  Other Cluster  Genesys International Corporation Ltd\n",
      "  Other Cluster       Indian Infotech and Software Ltd\n",
      "  Other Cluster                 Cressanda Solution Ltd\n",
      "  Other Cluster                  Xtglobal Infotech Ltd\n",
      "\n",
      "Results saved to ../Json_File/Refined_file/Clustered_Companies.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the data\n",
    "file_path = \"../Json_File/Refined_file/Updated_Filtered_Quarterly_data.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to determine the number of digits in a number (ignoring decimals and negatives)\n",
    "def get_digit_count(value):\n",
    "    abs_value = abs(value)  # Handle negative numbers\n",
    "    int_part = int(abs_value)  # Remove decimal part\n",
    "    if int_part == 0:\n",
    "        return 1  # Special case for values like 0.23\n",
    "    return len(str(int_part))\n",
    "\n",
    "# Function to classify companies based on dominant digit count\n",
    "def cluster_companies_by_digits(data):\n",
    "    net_profit_data = data[\"Quarterly\"][\"Net profit/(loss) for the period\"]\n",
    "    \n",
    "    # Clusters\n",
    "    two_digit_cluster = []\n",
    "    three_digit_cluster = []\n",
    "    four_digit_cluster = []\n",
    "    other_cluster = []  # For companies that don’t fit neatly (e.g., mostly 1 digit or mixed)\n",
    "\n",
    "    # Threshold: At least 70% of values must fall in a digit category to be \"consistent\"\n",
    "    threshold = 0.7\n",
    "\n",
    "    for company, profits in net_profit_data.items():\n",
    "        digit_counts = defaultdict(int)\n",
    "        total_values = len(profits)\n",
    "\n",
    "        # Count digits for each profit value\n",
    "        for profit in profits:\n",
    "            digit_count = get_digit_count(profit)\n",
    "            digit_counts[digit_count] += 1\n",
    "\n",
    "        # Calculate the dominant digit count\n",
    "        max_digit = max(digit_counts, key=digit_counts.get)\n",
    "        max_count = digit_counts[max_digit]\n",
    "        proportion = max_count / total_values\n",
    "\n",
    "        # Cluster based on dominant digit count and consistency\n",
    "        if proportion >= threshold:  # Must be consistent (70% or more in one category)\n",
    "            if max_digit == 2:\n",
    "                two_digit_cluster.append(company)\n",
    "            elif max_digit == 3:\n",
    "                three_digit_cluster.append(company)\n",
    "            elif max_digit == 4:\n",
    "                four_digit_cluster.append(company)\n",
    "            else:\n",
    "                other_cluster.append(company)\n",
    "        else:\n",
    "            other_cluster.append(company)  # Mixed or inconsistent values\n",
    "\n",
    "    return {\n",
    "        \"2-Digit Cluster\": two_digit_cluster,\n",
    "        \"3-Digit Cluster\": three_digit_cluster,\n",
    "        \"4-Digit Cluster\": four_digit_cluster,\n",
    "        \"Other Cluster\": other_cluster\n",
    "    }\n",
    "\n",
    "# Run the clustering\n",
    "clusters = cluster_companies_by_digits(data)\n",
    "\n",
    "# Prepare data for pandas DataFrame\n",
    "cluster_data = {\n",
    "    \"Cluster\": [],\n",
    "    \"Company\": []\n",
    "}\n",
    "\n",
    "# Populate the data for the DataFrame\n",
    "for cluster_name, companies in clusters.items():\n",
    "    for company in companies:\n",
    "        cluster_data[\"Cluster\"].append(cluster_name)\n",
    "        cluster_data[\"Company\"].append(company)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(cluster_data)\n",
    "\n",
    "# Sort the DataFrame by Cluster for better readability\n",
    "df = df.sort_values(by=\"Cluster\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Companies Clustered by Net Profit Digits:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Optionally, save to a CSV file\n",
    "output_file = \"../Json_File/Refined_file/Clustered_Companies.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies Clustered by Dominant Net Profit Digits:\n",
      "        Cluster                                Company\n",
      "1-Digit Cluster                               IZMO Ltd\n",
      "1-Digit Cluster Equippp Social Impact Technologies Ltd\n",
      "1-Digit Cluster                   Expleo Solutions Ltd\n",
      "1-Digit Cluster             FCS Software Solutions Ltd\n",
      "1-Digit Cluster  Genesys International Corporation Ltd\n",
      "1-Digit Cluster       Indian Infotech and Software Ltd\n",
      "1-Digit Cluster               Inspirisys Solutions Ltd\n",
      "1-Digit Cluster                      Softsol India Ltd\n",
      "1-Digit Cluster       Jeevan Scientific Technology Ltd\n",
      "1-Digit Cluster                 Elnet Technologies Ltd\n",
      "1-Digit Cluster              Kati Patang Lifestyle Ltd\n",
      "1-Digit Cluster        Kernex Microsystems (India) Ltd\n",
      "1-Digit Cluster                             Mastek Ltd\n",
      "1-Digit Cluster                           Megasoft Ltd\n",
      "1-Digit Cluster                           Nettlinx Ltd\n",
      "1-Digit Cluster                Onward Technologies Ltd\n",
      "1-Digit Cluster                Palred Technologies Ltd\n",
      "1-Digit Cluster                 Saven Technologies Ltd\n",
      "1-Digit Cluster                          SBSJHVHSV Ltd\n",
      "1-Digit Cluster             Kellton Tech Solutions Ltd\n",
      "1-Digit Cluster     Dynacons Systems and Solutions Ltd\n",
      "1-Digit Cluster         Datamatics Global Services Ltd\n",
      "1-Digit Cluster          Danlaw Technologies India Ltd\n",
      "1-Digit Cluster                  Xtglobal Infotech Ltd\n",
      "1-Digit Cluster                Xchanging Solutions Ltd\n",
      "1-Digit Cluster                      WEP Solutions Ltd\n",
      "1-Digit Cluster                   VEDAVAAG Systems Ltd\n",
      "1-Digit Cluster                 USG Tech Solutions Ltd\n",
      "1-Digit Cluster                Trigyn Technologies Ltd\n",
      "1-Digit Cluster                       Titan Intech Ltd\n",
      "1-Digit Cluster                      Tera Software Ltd\n",
      "1-Digit Cluster     Starcom Information Technology Ltd\n",
      "1-Digit Cluster                    Adroit Infotech Ltd\n",
      "1-Digit Cluster                AION-TECH SOLUTIONS Ltd\n",
      "1-Digit Cluster            Allied Digital Services Ltd\n",
      "1-Digit Cluster                   ASM Technologies Ltd\n",
      "1-Digit Cluster                Avance Technologies Ltd\n",
      "1-Digit Cluster   Cambridge Technology Enterprises Ltd\n",
      "1-Digit Cluster        CG-Vak Software and Exports Ltd\n",
      "1-Digit Cluster      Cranes Software International Ltd\n",
      "1-Digit Cluster                 Cressanda Solution Ltd\n",
      "1-Digit Cluster     Cybertech Systems and Software Ltd\n",
      "1-Digit Cluster           Securekloud Technologies Ltd\n",
      "1-Digit Cluster                     Sofcom Systems Ltd\n",
      "2-Digit Cluster                        3i Infotech Ltd\n",
      "2-Digit Cluster                Zensar Technologies Ltd\n",
      "2-Digit Cluster                    Sonata Software Ltd\n",
      "2-Digit Cluster                Sasken Technologies Ltd\n",
      "2-Digit Cluster            R Systems International Ltd\n",
      "2-Digit Cluster                 Persistent Systems Ltd\n",
      "2-Digit Cluster                             Cyient Ltd\n",
      "2-Digit Cluster                    COFORGE LIMITED Ltd\n",
      "2-Digit Cluster                          Birlasoft Ltd\n",
      "2-Digit Cluster                   Bartronics India Ltd\n",
      "3-Digit Cluster                            MphasiS Ltd\n",
      "3-Digit Cluster Oracle Financial Services Software Ltd\n",
      "3-Digit Cluster                      Tech Mahindra Ltd\n",
      "4-Digit Cluster                   HCL Technologies Ltd\n",
      "4-Digit Cluster                            Infosys Ltd\n",
      "4-Digit Cluster          Tata Consultancy Services Ltd\n",
      "4-Digit Cluster                              Wipro Ltd\n",
      "\n",
      "Results saved to ../Json_File/Refined_file/Dynamic_Clustered_Companies.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the data\n",
    "file_path = \"../Json_File/Refined_file/Updated_Filtered_Quarterly_data.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to determine the number of digits in a number (ignoring decimals and negatives)\n",
    "def get_digit_count(value):\n",
    "    abs_value = abs(value)  # Handle negative numbers\n",
    "    int_part = int(abs_value)  # Remove decimal part\n",
    "    if int_part == 0:\n",
    "        return 1  # Special case for values like 0.23\n",
    "    return len(str(int_part))\n",
    "\n",
    "# Function to cluster companies by dominant digit count\n",
    "def cluster_companies_by_digits(data):\n",
    "    net_profit_data = data[\"Quarterly\"][\"Net profit/(loss) for the period\"]\n",
    "    \n",
    "    # Dictionary to hold clusters dynamically (key: digit count, value: list of companies)\n",
    "    clusters = defaultdict(list)\n",
    "\n",
    "    for company, profits in net_profit_data.items():\n",
    "        digit_counts = defaultdict(int)\n",
    "        total_values = len(profits)\n",
    "\n",
    "        # Count digits for each profit value\n",
    "        for profit in profits:\n",
    "            digit_count = get_digit_count(profit)\n",
    "            digit_counts[digit_count] += 1\n",
    "\n",
    "        # Find the dominant digit count (most frequent)\n",
    "        dominant_digit = max(digit_counts, key=digit_counts.get)\n",
    "        \n",
    "        # Add the company to the cluster for its dominant digit count\n",
    "        clusters[dominant_digit].append(company)\n",
    "\n",
    "    # Convert defaultdict to regular dict with descriptive keys\n",
    "    clustered_result = {}\n",
    "    for digit_count, companies in clusters.items():\n",
    "        clustered_result[f\"{digit_count}-Digit Cluster\"] = companies\n",
    "\n",
    "    return clustered_result\n",
    "\n",
    "# Run the clustering\n",
    "clusters = cluster_companies_by_digits(data)\n",
    "\n",
    "# Prepare data for pandas DataFrame (still used for display)\n",
    "cluster_data = {\n",
    "    \"Cluster\": [],\n",
    "    \"Company\": []\n",
    "}\n",
    "\n",
    "# Populate the data for the DataFrame\n",
    "for cluster_name, companies in clusters.items():\n",
    "    for company in companies:\n",
    "        cluster_data[\"Cluster\"].append(cluster_name)\n",
    "        cluster_data[\"Company\"].append(company)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(cluster_data)\n",
    "\n",
    "# Sort the DataFrame by Cluster for better readability\n",
    "df = df.sort_values(by=\"Cluster\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Companies Clustered by Dominant Net Profit Digits:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save to a JSON file instead of CSV\n",
    "output_file = \"../Json_File/Refined_file/Dynamic_Clustered_Companies.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(clusters, file, indent=4)  # Save the clusters dict directly as JSON\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing metadata (1 companies):\n",
      "\n",
      "metadata: Not enough data to train model (only 0 samples).\n",
      "  Skipping model training due to insufficient data.\n",
      "\n",
      "Processing 1-Digit Cluster (44 companies):\n",
      "Model for 1-Digit Cluster:\n",
      "  Mean Squared Error (MSE): 89.38\n",
      "  R² Score: 0.36\n",
      "\n",
      "Processing 2-Digit Cluster (10 companies):\n",
      "Model for 2-Digit Cluster:\n",
      "  Mean Squared Error (MSE): 2295.25\n",
      "  R² Score: 0.24\n",
      "\n",
      "Processing 3-Digit Cluster (3 companies):\n",
      "Model for 3-Digit Cluster:\n",
      "  Mean Squared Error (MSE): 180.49\n",
      "  R² Score: 1.00\n",
      "\n",
      "Processing 4-Digit Cluster (4 companies):\n",
      "Model for 4-Digit Cluster:\n",
      "  Mean Squared Error (MSE): 3234.60\n",
      "  R² Score: 1.00\n",
      "\n",
      "All clusters processed!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the quarterly data\n",
    "data_file_path = \"../Json_File/Refined_file/Updated_Filtered_Quarterly_data.json\"\n",
    "with open(data_file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Load the clustered companies\n",
    "cluster_file_path = r\"C:\\Users\\sharm\\OneDrive\\Desktop\\Kishan\\Contractzy\\WebScrapping\\Tutorial\\Contractzy\\Company_info\\Dynamic_Clustered_Companies.json\"  # Adjust if path differs\n",
    "with open(cluster_file_path, \"r\") as file:\n",
    "    clusters = json.load(file)\n",
    "\n",
    "# Get all parameters (excluding \"Quarters\")\n",
    "all_parameters = list(data[\"Quarterly\"].keys())[1:]  # Skip \"Quarters\"\n",
    "\n",
    "# Define independent and dependent variables\n",
    "independent_vars = [\n",
    "    \"Total income from operations\", \"Employees cost\",\n",
    "    \"depreciat\", \"Other expenses\", \"P/l before other inc. , int., excpt. items & tax\",\n",
    "    \"Other income\", \"P/l before int., excpt. items & tax\", \"Interest\",\n",
    "    \"P/l before exceptional items & tax\", \"Tax\"\n",
    "]\n",
    "dependent_var = \"Net profit/(loss) for the period\"\n",
    "\n",
    "# Check if all required columns exist\n",
    "missing_columns = [col for col in independent_vars + [dependent_var] if col not in all_parameters]\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns in dataset: {missing_columns}\")\n",
    "    exit()\n",
    "\n",
    "# Function to combine data for a list of companies\n",
    "def combine_data_for_companies(company_list, data):\n",
    "    param_data = {param: [] for param in all_parameters}\n",
    "    \n",
    "    for param in all_parameters:\n",
    "        parameter_data = data[\"Quarterly\"][param]\n",
    "        combined_values = []\n",
    "        for company in company_list:\n",
    "            if company in parameter_data:\n",
    "                values = parameter_data[company]\n",
    "                if isinstance(values, list):\n",
    "                    combined_values.extend(values)\n",
    "        param_data[param] = combined_values\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in param_data.items()]))\n",
    "    return df\n",
    "\n",
    "# Function to train and evaluate MLR model\n",
    "def train_mlr_model(df, cluster_name):\n",
    "    # Prepare X (independent) and y (dependent)\n",
    "    X = df[independent_vars]\n",
    "    y = df[dependent_var]\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    X = X.dropna()\n",
    "    y = y.loc[X.index].dropna()\n",
    "    X = X.loc[y.index]  # Align X with y after dropping NaNs in y\n",
    "\n",
    "    if len(X) < 2:  # Need at least 2 samples for train-test split\n",
    "        print(f\"\\n{cluster_name}: Not enough data to train model (only {len(X)} samples).\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Convert to numeric\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    y = pd.to_numeric(y, errors='coerce')\n",
    "\n",
    "    # Drop any remaining NaNs after conversion\n",
    "    X = X.dropna()\n",
    "    y = y.loc[X.index].dropna()\n",
    "    X = X.loc[y.index]\n",
    "\n",
    "    if len(X) < 2:\n",
    "        print(f\"\\n{cluster_name}: Not enough valid numeric data after cleaning (only {len(X)} samples).\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return model, mse, r2\n",
    "\n",
    "# Train and evaluate model for each cluster\n",
    "for cluster_name, companies in clusters.items():\n",
    "    print(f\"\\nProcessing {cluster_name} ({len(companies)} companies):\")\n",
    "    \n",
    "    # Combine data for this cluster\n",
    "    cluster_df = combine_data_for_companies(companies, data)\n",
    "    \n",
    "    # Train MLR model\n",
    "    model, mse, r2 = train_mlr_model(cluster_df, cluster_name)\n",
    "    \n",
    "    if model is not None:\n",
    "        print(f\"Model for {cluster_name}:\")\n",
    "        print(f\"  Mean Squared Error (MSE): {mse:.2f}\")\n",
    "        print(f\"  R² Score: {r2:.2f}\")\n",
    "    else:\n",
    "        print(f\"  Skipping model training due to insufficient data.\")\n",
    "\n",
    "print(\"\\nAll clusters processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training a single MLR model on all combined data...\n",
      "Global model training complete!\n",
      "\n",
      "Evaluating global model on 1-Digit Cluster (44 companies):\n",
      "  Mean Squared Error (MSE): 778.70\n",
      "  R² Score: -5.89\n",
      "\n",
      "Evaluating global model on 2-Digit Cluster (10 companies):\n",
      "  Mean Squared Error (MSE): 3962.86\n",
      "  R² Score: 0.57\n",
      "\n",
      "Evaluating global model on 3-Digit Cluster (3 companies):\n",
      "  Mean Squared Error (MSE): 1609.33\n",
      "  R² Score: 0.99\n",
      "\n",
      "Evaluating global model on 4-Digit Cluster (4 companies):\n",
      "  Mean Squared Error (MSE): 17175.58\n",
      "  R² Score: 1.00\n",
      "\n",
      "Evaluation complete for all clusters!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the quarterly data\n",
    "data_file_path = \"../Json_File/Refined_file/Updated_Filtered_Quarterly_data.json\"\n",
    "with open(data_file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Load the clustered companies\n",
    "cluster_file_path = \"../Json_File/Refined_file/Dynamic_Clustered_Companies.json\"\n",
    "with open(cluster_file_path, \"r\") as file:\n",
    "    clusters = json.load(file)\n",
    "\n",
    "# Get all parameters (excluding \"Quarters\")\n",
    "all_parameters = list(data[\"Quarterly\"].keys())[1:]  # Skip \"Quarters\"\n",
    "\n",
    "# Define independent and dependent variables\n",
    "independent_vars = [\n",
    "    \"Net sales/income from operations\", \"Total income from operations\", \"Employees cost\",\n",
    "    \"depreciat\", \"Other expenses\", \"P/l before other inc. , int., excpt. items & tax\",\n",
    "    \"Other income\", \"P/l before int., excpt. items & tax\", \"Interest\",\n",
    "    \"P/l before exceptional items & tax\", \"Tax\"\n",
    "]\n",
    "dependent_var = \"Net profit/(loss) for the period\"\n",
    "\n",
    "# Check if all required columns exist\n",
    "missing_columns = [col for col in independent_vars + [dependent_var] if col not in all_parameters]\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns in dataset: {missing_columns}\")\n",
    "    exit()\n",
    "\n",
    "# Function to combine data for a list of companies\n",
    "def combine_data_for_companies(company_list, data):\n",
    "    param_data = {param: [] for param in all_parameters}\n",
    "    \n",
    "    for param in all_parameters:\n",
    "        parameter_data = data[\"Quarterly\"][param]\n",
    "        combined_values = []\n",
    "        for company in company_list:\n",
    "            if company in parameter_data:\n",
    "                values = parameter_data[company]\n",
    "                if isinstance(values, list):\n",
    "                    combined_values.extend(values)\n",
    "        param_data[param] = combined_values\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in param_data.items()]))\n",
    "    return df\n",
    "\n",
    "# Combine data for all companies across all clusters\n",
    "all_companies = [company for companies in clusters.values() for company in companies]\n",
    "combined_df = combine_data_for_companies(all_companies, data)\n",
    "\n",
    "# Prepare and train one global MLR model\n",
    "print(\"\\nTraining a single MLR model on all combined data...\")\n",
    "\n",
    "# Prepare X (independent) and y (dependent) for the combined data\n",
    "X = combined_df[independent_vars]\n",
    "y = combined_df[dependent_var]\n",
    "\n",
    "# Drop rows with missing values\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index].dropna()\n",
    "X = X.loc[y.index]\n",
    "\n",
    "if len(X) < 2:\n",
    "    print(\"Not enough data to train model across all clusters.\")\n",
    "    exit()\n",
    "\n",
    "# Convert to numeric\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "y = pd.to_numeric(y, errors='coerce')\n",
    "\n",
    "# Drop any remaining NaNs after conversion\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index].dropna()\n",
    "X = X.loc[y.index]\n",
    "\n",
    "if len(X) < 2:\n",
    "    print(\"Not enough valid numeric data after cleaning.\")\n",
    "    exit()\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split for the combined data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the global model\n",
    "global_model = LinearRegression()\n",
    "global_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Global model training complete!\")\n",
    "\n",
    "# Function to evaluate the global model on a specific cluster's data\n",
    "def evaluate_model_on_cluster(cluster_name, companies, model, data):\n",
    "    # Combine data for this cluster\n",
    "    cluster_df = combine_data_for_companies(companies, data)\n",
    "    \n",
    "    # Prepare X and y for this cluster\n",
    "    X_cluster = cluster_df[independent_vars]\n",
    "    y_cluster = cluster_df[dependent_var]\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    X_cluster = X_cluster.dropna()\n",
    "    y_cluster = y_cluster.loc[X_cluster.index].dropna()\n",
    "    X_cluster = X_cluster.loc[y_cluster.index]\n",
    "\n",
    "    if len(X_cluster) < 1:\n",
    "        print(f\"\\n{cluster_name}: No valid data to evaluate.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert to numeric\n",
    "    X_cluster = X_cluster.apply(pd.to_numeric, errors='coerce')\n",
    "    y_cluster = pd.to_numeric(y_cluster, errors='coerce')\n",
    "\n",
    "    # Drop any remaining NaNs\n",
    "    X_cluster = X_cluster.dropna()\n",
    "    y_cluster = y_cluster.loc[X_cluster.index].dropna()\n",
    "    X_cluster = X_cluster.loc[y_cluster.index]\n",
    "\n",
    "    if len(X_cluster) < 1:\n",
    "        print(f\"\\n{cluster_name}: No valid numeric data after cleaning.\")\n",
    "        return None, None\n",
    "\n",
    "    # Standardize features using the same scaler as the global model\n",
    "    X_cluster_scaled = scaler.transform(X_cluster)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_cluster = model.predict(X_cluster_scaled)\n",
    "\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_cluster, y_pred_cluster)\n",
    "    r2 = r2_score(y_cluster, y_pred_cluster)\n",
    "\n",
    "    return mse, r2\n",
    "\n",
    "# Evaluate the global model on each cluster separately\n",
    "for cluster_name, companies in clusters.items():\n",
    "    print(f\"\\nEvaluating global model on {cluster_name} ({len(companies)} companies):\")\n",
    "    \n",
    "    mse, r2 = evaluate_model_on_cluster(cluster_name, companies, global_model, data)\n",
    "    \n",
    "    if mse is not None and r2 is not None:\n",
    "        print(f\"  Mean Squared Error (MSE): {mse:.2f}\")\n",
    "        print(f\"  R² Score: {r2:.2f}\")\n",
    "    else:\n",
    "        print(f\"  Skipping evaluation due to insufficient data.\")\n",
    "\n",
    "print(\"\\nEvaluation complete for all clusters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 1-Digit Cluster (44 companies):\n",
      "Model for 1-Digit Cluster:\n",
      "  Mean Squared Error (MSE): 89.38\n",
      "  R² Score: 0.36\n",
      "\n",
      "Processing 2-Digit Cluster (10 companies):\n",
      "Model for 2-Digit Cluster:\n",
      "  Mean Squared Error (MSE): 2295.26\n",
      "  R² Score: 0.24\n",
      "\n",
      "Processing 3-Digit Cluster (3 companies):\n",
      "Model for 3-Digit Cluster:\n",
      "  Mean Squared Error (MSE): 180.49\n",
      "  R² Score: 1.00\n",
      "\n",
      "Processing 4-Digit Cluster (4 companies):\n",
      "Model for 4-Digit Cluster:\n",
      "  Mean Squared Error (MSE): 3278.67\n",
      "  R² Score: 1.00\n",
      "\n",
      "All clusters processed!\n",
      "\n",
      "Average R² Score across all trained models: 0.65\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the quarterly data\n",
    "data_file_path = \"../Json_File/Refined_file/Updated_Filtered_Quarterly_data.json\"\n",
    "with open(data_file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Load the clustered companies\n",
    "cluster_file_path = \"../Json_File/Refined_file/Dynamic_Clustered_Companies.json\"\n",
    "with open(cluster_file_path, \"r\") as file:\n",
    "    clusters = json.load(file)\n",
    "\n",
    "# Get all parameters (excluding \"Quarters\")\n",
    "all_parameters = list(data[\"Quarterly\"].keys())[1:]  # Skip \"Quarters\"\n",
    "\n",
    "# Define independent and dependent variables\n",
    "independent_vars = [\n",
    "    \"Net sales/income from operations\", \"Total income from operations\", \"Employees cost\",\n",
    "    \"depreciat\", \"Other expenses\", \"P/l before other inc. , int., excpt. items & tax\",\n",
    "    \"Other income\", \"P/l before int., excpt. items & tax\", \"Interest\",\n",
    "    \"P/l before exceptional items & tax\", \"Tax\"\n",
    "]\n",
    "dependent_var = \"Net profit/(loss) for the period\"\n",
    "\n",
    "# Check if all required columns exist\n",
    "missing_columns = [col for col in independent_vars + [dependent_var] if col not in all_parameters]\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns in dataset: {missing_columns}\")\n",
    "    exit()\n",
    "\n",
    "# Function to combine data for a list of companies\n",
    "def combine_data_for_companies(company_list, data):\n",
    "    param_data = {param: [] for param in all_parameters}\n",
    "    \n",
    "    for param in all_parameters:\n",
    "        parameter_data = data[\"Quarterly\"][param]\n",
    "        combined_values = []\n",
    "        for company in company_list:\n",
    "            if company in parameter_data:\n",
    "                values = parameter_data[company]\n",
    "                if isinstance(values, list):\n",
    "                    combined_values.extend(values)\n",
    "        param_data[param] = combined_values\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in param_data.items()]))\n",
    "    return df\n",
    "\n",
    "# Function to train and evaluate MLR model\n",
    "def train_mlr_model(df, cluster_name):\n",
    "    # Prepare X (independent) and y (dependent)\n",
    "    X = df[independent_vars]\n",
    "    y = df[dependent_var]\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    X = X.dropna()\n",
    "    y = y.loc[X.index].dropna()\n",
    "    X = X.loc[y.index]  # Align X with y after dropping NaNs in y\n",
    "\n",
    "    if len(X) < 2:  # Need at least 2 samples for train-test split\n",
    "        print(f\"\\n{cluster_name}: Not enough data to train model (only {len(X)} samples).\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Convert to numeric\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    y = pd.to_numeric(y, errors='coerce')\n",
    "\n",
    "    # Drop any remaining NaNs after conversion\n",
    "    X = X.dropna()\n",
    "    y = y.loc[X.index].dropna()\n",
    "    X = X.loc[y.index]\n",
    "\n",
    "    if len(X) < 2:\n",
    "        print(f\"\\n{cluster_name}: Not enough valid numeric data after cleaning (only {len(X)} samples).\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return model, mse, r2\n",
    "\n",
    "# Dictionary to store models and their metrics\n",
    "cluster_models = {}\n",
    "r2_scores = []\n",
    "\n",
    "# Train and evaluate model for each cluster\n",
    "for cluster_name, companies in clusters.items():\n",
    "    print(f\"\\nProcessing {cluster_name} ({len(companies)} companies):\")\n",
    "    \n",
    "    # Combine data for this cluster\n",
    "    cluster_df = combine_data_for_companies(companies, data)\n",
    "    \n",
    "    # Train MLR model\n",
    "    model, mse, r2 = train_mlr_model(cluster_df, cluster_name)\n",
    "    \n",
    "    if model is not None:\n",
    "        print(f\"Model for {cluster_name}:\")\n",
    "        print(f\"  Mean Squared Error (MSE): {mse:.2f}\")\n",
    "        print(f\"  R² Score: {r2:.2f}\")\n",
    "        cluster_models[cluster_name] = model\n",
    "        r2_scores.append(r2)\n",
    "    else:\n",
    "        print(f\"  Skipping model training due to insufficient data.\")\n",
    "\n",
    "print(\"\\nAll clusters processed!\")\n",
    "\n",
    "# Calculate and display the average R² score\n",
    "if r2_scores:\n",
    "    average_r2 = sum(r2_scores) / len(r2_scores)\n",
    "    print(f\"\\nAverage R² Score across all trained models: {average_r2:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo models were trained successfully, so no average R² score to calculate.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
